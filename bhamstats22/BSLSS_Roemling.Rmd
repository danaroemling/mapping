---
title: "Mapping Swearing"
subtitle: "Birmingham Statistics for Linguists Summer School"
author: "Dana Roemling"
date: "17/05/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy = TRUE)
```

### Introduction

This tutorial provides a brief introduction into mapping linguistic data using R. That means we'll be working with regional data and we'll want to map features to help us understand regional distribution of language varieties. This can be useful for dialectology, but is also used in dialectometry and NLP approaches. 


### First Steps

In preparation for our maps, we'll need to load a couple of packages. For mapping we'll need 'maps' and for some optional maps 'rworldmap', which also loads 'sp'. Depending on your version of R you might also need 'broom'. The main work for the maps will be done using ggplot, so we need the tidyverse package as well. 

```{r prep, message=FALSE}
library(maps) # to get US maps
library(rworldmap) # mapping other country outlines
library(tidyverse) # making pretty maps
library(sf) # to change the geo-information to suitable format
```


#### Data

The data we'll be using is based on a collection of 1 billion Tweets / 9 billion words. All Tweets are geocoded American Tweets collected between 2013 and 2014. From this a US Twitter swearing dataset was compiled. See [Huang et al. 2016](http://pure-oai.bham.ac.uk/ws/files/44758092/1_s2.0_S0198971515300399_main.pdf); and [Grieve et al. 2017](
https://doi.org/10.1017/S1360674316000113) for more information. 

The initial step now is reading in the dataset.

```{r data1, message=FALSE}
norm_swear <- read.table("BSLSS_SWEAR.txt", header = TRUE, sep = ",")
```

The basic dimension of the dataset is 52 swear words measured across 3,085 locations, denoted by state plus county (= 53).
```{r data1.1, message=FALSE}
dim(norm_swear)
```

The locations are coded as state-county pairs. These are the first 15 rows of our dataset.
```{r data1.2, message=FALSE}
head(norm_swear, 15)
```

For each county, Grieve et al. (2017) measured the relative frequency per billion words of the word in all the Tweets originating from that county by dividing the frequency of that word in those Tweets by the total number of words in those Tweets and multiplying the product by 1 billion. These swear words are all in the top 10,000 most frequent word types in the corpus. Here is a summary of the swear words.

```{r data1.3, message=FALSE}
summary(norm_swear[, 2:ncol(norm_swear)])
```


### Mapping

Before we map our swearing data, we need to understand the basics of cartography in R.

#### Mapping the US

First, we need to get a map of the US, which we will format and use as a base to plot our swear word relative frequencies on to. There are several stages to setting up a nice map. Aside from the first step though, which just involves reading in the underlying map, they're all optional. 

#### Accessing Mapping Data in R

First, we need to get a US map. Fortunately working with US data is very easy in R, since all the necessary maps can be accessed in library(maps). We use ggplot's map_data function to extract the relevant information from the package. 

```{r us_map1, message=FALSE}
usa <- map_data("usa")
```

Now we'll have a look at the very basic US map. For this we'll need ggplot:

```{r us_map2}
ggplot() + 
  geom_polygon(data = usa, 
               aes(x = long, y = lat, group = region))
```


### Other countries

If you want to map other countries, you can download and read in the base mapping data (e.g. shapefiles), which are available from various different sources. This is especially interesting if you're looking to work with administrative regions and the like. For country outlines, you can also use library(rworldmaps). This example below shows how to produce a map of Germany, Austria and Switzerland (= German-Speaking Area, GSA). rworldmap works with coordinates. 

What this code chunk does is getting the world map and then creating a list of three countries by name. Then we create a map based on that list and in the next step we get the coordinates of those countries, so that we can use these for mapping.

```{r worldmap1, message=FALSE}
worldMap <- getMap()
GSA <- c("Germany", "Austria", "Switzerland")
GSA_map <- which(worldMap$NAME%in%GSA)
GSA_coord <- lapply(GSA_map, function(i){
  df <- data.frame(worldMap@polygons[[i]]@Polygons[[1]]@coords)
  df$region = as.character(worldMap$NAME[i])
  colnames(df) <- list("long", "lat", "region")
  return(df)
})
GSA_coord <- do.call("rbind", GSA_coord)
```

After this, we can have a look at our three countries using ggplot. The coord_fixed argument makes sure that the relationship between x and y is correct; it fixes the aspect ratio.

```{r gsa_map}
gsa_map <- ggplot() + 
  geom_polygon(data = GSA_coord, 
               aes(x = long, y = lat, group = region)) +
  # this bit does the aspect ratio fix
  coord_fixed(1.3) 
gsa_map
```

#### Back to the US

Now, we need to make sure our US data can be mapped, which means we don't just need the outline of the US, but we need the counties. We can extract them from our maps package.

```{r us_counties}
counties <- map_data("county")
ggplot() + 
  geom_polygon(data = counties, 
               aes(x = long, y = lat, group = group),
               # to see the counties we add a colour for outline and filling
               color = "black", fill = "lightgrey", 
               size = .1 ) +
  coord_fixed(1.3)
```

#### Polishing our map

Now that we have a basic map of the US, we can make it look a bit nicer, so that subsequent maps are easier to read.

```{r design}
ggplot() + 
  geom_polygon(data = counties, 
               aes(x = long, y = lat, group = group),
               color = "black", fill = "white", 
               size = .1 ) +
  coord_fixed(1.3) +
  theme_minimal() +  # sets the theme for the plot
  ggtitle("US Map with Counties") + # gives the plot a title
  theme(axis.title.x = element_blank(), # removes x axis title, here longitude
        axis.title.y = element_blank(),# removes y axis title, here latitude
        axis.text.x = element_blank(), # removes x axis text, here coordinates
        axis.text.y = element_blank(), # removes y axis text, here coordinates
        panel.grid.major = element_blank(), # removes grid lines
        panel.grid.minor = element_blank(), # removes grid lines
        plot.title = element_text(hjust = 0.5)) # centres title
```

### Data Wrangling

Now that we have a base map and our data read in, we need to make sure the data can be mapped. This might look a bit complicated, but what we're doing is getting the coordinate data that we need to join our existing dataset. 

First, we get a map of the counties (aka the geo-information we need) and save it as us_geo (and have a little look, colourful!). For this we need the package 'sf'. We're still using the same "maps" library as before, but since each county has multiple sets of coordinates, we need a format that can be matched to our dataset, where each location is just one row, hence we're handling it with 'sf'. We merge the two separate lists into one using dplyr. 

```{r merge, message=FALSE}
us_geo <- st_as_sf(maps::map(database = "county", 
                             plot = FALSE, 
                             fill = TRUE))
plot(us_geo)
us_geo_swear <- us_geo %>%
  dplyr::left_join(norm_swear, 
            by = c("ID" = "county"))
```

If you have a look at the new data frame us_geo_swear, you can see that it is essentially the same list as before, but that the last column contains another list, as every county has multiple coordinate points, which we need for plotting.

```{r data_geo}
# shows us that it is a data frame
class(us_geo_swear)
# you can see that we now have a data frame that contains multipolygons
head(us_geo_swear) 
# If you open the data frame and scroll to the last column, 
# you can see the list in the list.
view(us_geo_swear) 
```

Now that the data is prepared, we can try and map some swear words. Note that we've added geom_sf to the plot. We do this because it can handle the sf data we've added for the geolocation of our swear words. That also means we don't need geom_polygon, but by the name you can tell it has similar functionality. 

This first map is a very basic choropleth map based on our variable "ass":

```{r full_map1.0}
ggplot() +
  geom_sf(data = us_geo_swear, 
          aes(fill = ass)) 
```

Let's add our design to it:

```{r full_map1.1}
ggplot() +
  geom_sf(data = us_geo_swear, 
          aes(fill = ass)) +
  theme_minimal() +  
  ggtitle("'Ass' Distribution in the US per County") + 
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_blank(), 
        axis.text.y = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.title = element_text(hjust = 0.5)) 
```

That looks sort of like what we want, so let's rework it a bit. Note that we divide the occurrences of 'ass' by 10.000, since we're dealing with high numbers we can thus make our graph easier to read this way.

```{r full_map1.2}
ggplot() +
  geom_sf(data = us_geo_swear, 
          aes(fill = ass / 10000), 
          lwd = 0.1, # lwd sets the outline thickness of the polygons
          color = "grey") + # this sets the outline colour
  theme_minimal() +  
  ggtitle("'Ass' Distribution in the US per County") + 
  # this adds a new legend title with line break \n
  guides(fill = guide_legend(title = "Distribution \nin 10.000")) + 
  # here we start using some nicer colours
  scale_fill_continuous(low = "white", 
                        high = "mediumpurple4") + 
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_blank(), 
        axis.text.y = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.title = element_text(hjust = 0.5),
        legend.title = element_text(size = 8))
```

We can see that there seems to be a trend towards ass in the Southeast. 
Let's see if we can see some more trends.

```{r dickhead}
ggplot() +
  geom_sf(data = us_geo_swear, 
          aes(fill = dickhead / 10000), 
          lwd = 0.1, 
          color = "grey") + 
  theme_minimal() +  
  ggtitle("'Dickhead' Distribution in the US per County") + 
  guides(fill = guide_legend(title = "Distribution \nin 10.000")) + 
  scale_fill_continuous(low = "white", 
                        high = "mediumpurple4") + 
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_blank(), 
        axis.text.y = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.title = element_text(hjust = 0.5),
        legend.title = element_text(size = 8))
```

How about fuck, but in green?

```{r fuck}
ggplot() +
  geom_sf(data = us_geo_swear, 
          aes(fill = fuck / 10000), 
          lwd = 0.1, 
          color = "grey") + 
  theme_minimal() +  
  ggtitle("'Fuck' Distribution in the US per County") + 
  guides(fill = guide_legend(title = "Distribution \nin 10.000")) + 
  scale_fill_continuous(low = "white", 
                        high = "aquamarine4") + # green this time?
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_blank(), 
        axis.text.y = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.title = element_text(hjust = 0.5),
        legend.title= element_text(size = 8))
```


### Quantiles

In the next step for the swearing maps we'll implement quantiles. What that means is we split the relative frequency distribution for the word we want to map into intervals. We're using "quantile" style intervals here, where the values are split so each interval contains a roughly equal number of values, although the range of each interval will likely vary (often considerably).

In order to do this we'll first pick a swear word, it's location and create a new list. Then we'll calculate the quantiles for our swear word and add this as a factor to our list. Exchange the swear word in this code to run it with a different one.

```{r quantiles}
# select the columns you need
quant_swear <- us_geo_swear %>% 
  select(bitch, geom) 
# calculate quantiles
q <- quantile(quant_swear$bitch, 
              na.rm = TRUE) 
# add factor given the quantiles to our list
quant_swear$quant <- factor(findInterval(quant_swear$bitch, q)) 
```

Now we can map our data. Instead of filling the polygons by the frequency of our swear word, we use the quantiles we've just defined. Note that that means we're going from continuous scale colours to discrete, so we need to change the colouring option of our map. That's why we first define these colours.

```{r q_map}
cols <- c("1" = "white", 
          "2" = "lightsteelblue1", 
          "3" = "lightsteelblue2", 
          "4" = "lightsteelblue3", 
          "5" = "lightsteelblue4")
ggplot() +
  # we've added na.omit to not have NAs plotted 
  geom_sf(data = na.omit(quant_swear), 
          aes(fill = quant), 
          lwd = 0.1, 
          color = "grey") + 
  # here we pass our colour list
  scale_colour_manual(values = cols, 
                      #and say we use it to fill
                      aesthetics = c("colour", "fill")) + 
  theme_minimal() +  
  ggtitle("'Bitch' Quantile Distribution in the US") + 
  guides(fill = guide_legend(title = "Quantiles")) + 
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_blank(), 
        axis.text.y = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.title = element_text(hjust = 0.5),
        legend.title = element_text(size = 8))
```

Let's map the quantiles of another swear word and change the colours for the map.
If you want to play around with colour yourself, [this website](http://sape.inf.usi.ch/quick-reference/ggplot2/colour) offers a good overview.

```{r q_map2}
quant_swear <- us_geo_swear %>% select(shit, geom) 
q <- quantile(quant_swear$shit, na.rm = TRUE) 
quant_swear$quant <- factor(findInterval(quant_swear$shit,q)) 
cols <- c("1" = "white", 
          "2" = "rosybrown1", 
          "3" = "rosybrown2", 
          "4" = "rosybrown3", 
          "5" = "rosybrown4")
ggplot() +
  geom_sf(data = na.omit(quant_swear), 
          aes(fill = quant), 
          lwd = 0.1, 
          color = "grey") + 
  scale_colour_manual(values = cols, 
                      aesthetics = c("colour", "fill")) + 
  theme_minimal() +  
  ggtitle("'Shit' Quantile Distribution in the US") + 
  guides(fill = guide_legend(title = "Quantiles")) + 
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_blank(), 
        axis.text.y = element_blank(), 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.title = element_text(hjust = 0.5),
        legend.title = element_text(size = 8))
```


### Adding cities

As the last bit, we'll try out adding another layer to our ggplot maps. 
Remember our map for the German-speaking area.

```{r gsa_map2.1}
gsa_map
```

If we wanted to add cities to this, because we're interested in looking at a city level population, we can do this using geom_point. Let's first create some sample data to do this.

```{r gsa_sample}
gsa_data <- data.frame(
  City_name = c("Cologne", "Munich", "Vienna", "Bern", "Berlin", "Hamburg", "Kassel", "Graz"), 
  Count1 = c(19, 4, 2, 5, 10, 43, 18, 7), 
  Count2 = c(20, 5, 1, 3, 21, 57, 28, 4),
  Proportion = c(38.78, 44.44, 66.67, 62.5, 32.26, 43.0, 39.13, 63.64),
  Long = c(6.9578, 11.5755, 16.3731, 7.4474, 13.3833, 10, 9.4912, 15.4409),
  Lat = c(50.9422, 48.1372, 48.2083, 46.948, 52.5167, 53.55, 51.3166, 47.0749))
gsa_data
```

Note that we again have a dataset which contains both the linguistic information (here the counts and proportion) and the geolocation information. With this, we can map the data using the cities. 

First, we again use our coordinates to create the basic map of the GSA, just as we did before. Only in the geom_point layer do we add the city data.

```{r gsa_map2.2}
ggplot() + 
  geom_polygon(data = GSA_coord, 
               aes(x = long, y = lat, group = region),
               # sets outline and fill sets the filling of the GSA
               colour = "black", 
               size = 0.1, 
               fill = "snow3") + 
  coord_map(xlim = c(4.5, 17),  # this cuts the map to the coordinates we need
            ylim = c(45.5, 55)) + 
  theme_minimal() +  
  geom_point(data = gsa_data, # here we add the cities to our map
             aes(x = Long, y = Lat, col = Proportion, size = (Count1+Count2)), 
             alpha = 0.9)  +
  guides(size = FALSE) +
  scale_color_gradient(low = "seagreen3", high = "mediumpurple3") +
  ggtitle("Feature 1 vs Feature 2 in the GSA") +
  theme(axis.title.x = element_blank(), 
        axis.title.y = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        panel.grid.major = element_blank(),
        plot.title = element_text(hjust = 0.5))
```

What this map shows us is the proportion of usage of the two feature in the given cities. Germany shows more feature 1 use, whereas Austria and Switzerland tend to use more feature 2 in our made-up data. The proportion baseline is feature 1. The size of the cities is dependent on the occurrences of both features combined.


#### Saving your output

As the last step we want to save our map. 

```{r save, message=FALSE}
ggsave("germany_map.png", width = 6.5, height = 5.5)
```

